---
apiVersion: helm.fluxcd.io/v1
kind: HelmRelease
metadata:
  name: prometheus-operator
  namespace: monitoring
  annotations:
    fluxcd.io/ignore: "false"
    fluxcd.io/automated: "false"
spec:
  releaseName: prometheus-operator
  chart:
    repository: https://kubernetes-charts.storage.googleapis.com/
    name: prometheus-operator
    version: 8.7.0
  values:
    server:
      resources:
        requests:
          memory: 1000Mi
          cpu: 25m
        limits:
          memory: 2000Mi
    prometheusOperator:
      createCustomResource: true
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/worker
                operator: In
                values:
                - "true"
    alertmanager:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
      alertmanagerSpec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: In
                  values:
                  - "true"
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: rook-ceph-block
              resources:
                requests:
                  storage: 10Gi
    grafana:
      deploymentStrategy:
        type: Recreate
      podAnnotations:
        backup.velero.io/backup-volumes: storage
      persistence:
        enabled: true
        storageClassName: "rook-ceph-block"
        size: 10Gi
        accessModes:
        - ReadWriteOnce
      env:
        GF_EXPLORE_ENABLED: true
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
      plugins:
      - natel-discrete-panel
      - pr0ps-trackmap-panel
      - grafana-piechart-panel
      - vonage-status-panel
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          nginx-dashboard:
            url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/grafana/dashboards/nginx.json
            datasource: Prometheus
          ceph-cluster:
            url: https://grafana.com/api/dashboards/2842/revisions/11/download
            datasource: Prometheus
          ceph-osd:
            url: https://grafana.com/api/dashboards/5336/revisions/3/download
            datasource: Prometheus
          ceph-pools:
            url: https://grafana.com/api/dashboards/5342/revisions/3/download
            datasource: Prometheus
          velero:
            url: https://grafana.com/api/dashboards/11055/revisions/2/download
            datasource: Prometheus
          pihole-dashboard:
            url: https://raw.githubusercontent.com/eko/pihole-exporter/master/grafana/dashboard.json
            datasource: Prometheus
          flux:
            url: https://grafana.com/api/dashboards/10475/revisions/1/download
            datasource: Prometheus
      additionalDataSources:
      - name: loki
        type: loki
        access: proxy
        url: http://loki.logging.svc.cluster.local:3100
      grafana.ini:
        paths:
          data: /var/lib/grafana/data
          logs: /var/log/grafana
          plugins: /var/lib/grafana/plugins
          provisioning: /etc/grafana/provisioning
        analytics:
          check_for_updates: true
        log:
          mode: console
        grafana_net:
          url: https://grafana.net
    kubeEtcd:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeProxy:
      enabled: false
    prometheus:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
      prometheusSpec:
        affinity:
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
              - matchExpressions:
                - key: node-role.kubernetes.io/worker
                  operator: In
                  values:
                  - "true"
        ruleSelector: {}
        ruleNamespaceSelector: {}
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelector: {}
        podMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        retention: 30d
        enableAdminAPI: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: rook-ceph-block
              resources:
                requests:
                  storage: 100Gi
        additionalScrapeConfigs:
        - job_name: 'netdata-scrape'
          metrics_path: '/api/v1/allmetrics'
          params:
              # format: prometheus | prometheus_all_hosts
              # You can use `prometheus_all_hosts` if you want Prometheus to set the `instance` to your hostname instead of IP 
            format: [prometheus]
          honor_labels: true
          static_configs:
          - targets:
            - pihole:19999
        - job_name: 'pihole-exporter'
          static_configs:
          - targets: ['pihole-exporter.monitoring.svc.cluster.local:9617']
        - job_name: 'sonarr-exporter'
          static_configs:
          - targets: ['sonarr-exporter.monitoring.svc.cluster.local:9811']
  valueFileSecrets:
  - name: "prometheus-operator-helm-values"
